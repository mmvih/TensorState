import os
from pathlib import Path
import math

from scipy.integrate import quad
import multiprocessing, argparse, logging, matplotlib, copy, imageio
matplotlib.use('agg')
import matplotlib.pyplot as plt
from pathlib import Path
# from bitstring import BitArray
from pylab import figure, axes, pie, title, show
import binascii
import codecs 
import json
import zarr, abc, sys, logging
from numcodecs import Blosc
import TensorState.States
from scipy import stats
from scipy.stats import chi2_contingency
from mpl_toolkits import mplot3d
import matplotlib.pyplot as plt
from scipy.stats import multivariate_normal
from sklearn.feature_selection import mutual_info_classif
from sklearn.metrics import mutual_info_score
import pandas as pd


def shans_entropy(prob):
    entropy = -(prob * np.log2(prob))
    return entropy
# from keras.callbacks import LambdaCallback

# Set the log level to hide some basic warning/info generated by Tensorflow
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

# Fix for cudnn error on RTX gpus
os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'

import tensorflow as tf
import tensorflow.keras as keras
from tensorflow.keras.models import load_model
import TensorState as ts
import numpy as np
# np.set_printoptions(threshold=sys.maxsize)
import time
from TensorState import AbstractStateCapture as asc
from TensorState.Layers import StateCapture, StateCaptureHook
# import TensorState.States as tensorstate
import struct


""" Load MNIST and transform it """
# Load the data
mnist = keras.datasets.mnist
(train_images,train_labels), (test_images,test_labels) = mnist.load_data()

# Normalize the data
train_images = train_images/255
test_images = test_images/255

# Add a channel axis
train_images = train_images[..., tf.newaxis]
test_images = test_images[..., tf.newaxis]

test_images = test_images[0:100,:,:,:]
test_labels = test_labels[0:100]
testmin = np.amin(test_images)

# print("test minimum", testmin)
test_images = test_images - testmin
# print(test_images > 0)

print(test_images.shape)
print(test_labels.shape)
""" Create a LeNet-5 model """
# Set the random seed for reproducibility
tf.random.set_seed(0)

# Set the convolutional layer settings
reg = keras.regularizers.l2(0.0005)
kwargs = {'activation': 'elu',
        'kernel_initializer': 'he_normal',
        'kernel_regularizer': reg,
        'bias_regularizer': reg}

# Build the layers
input_layer = keras.layers.Input(shape=(28,28,1), name='input')

# Unit 1
conv_1 = keras.layers.Conv2D(20, 5, name='conv_1',**kwargs)(input_layer)
norm_1 = keras.layers.BatchNormalization(epsilon=0.00001,momentum=0.9)(conv_1)
maxp_1 = keras.layers.MaxPool2D((2,2), name='maxp_1')(norm_1)

# Unit 2
conv_2 = keras.layers.Conv2D(50, 5, name='conv_2', **kwargs)(maxp_1)
norm_2 = keras.layers.BatchNormalization(epsilon=0.00001,momentum=0.9)(conv_2)
maxp_2 = keras.layers.MaxPool2D((2,2), name='maxp_2')(norm_2)

# Fully Connected
conv_3 = keras.layers.Conv2D(100, 4, name='conv_3', **kwargs)(maxp_2)
norm_3 = keras.layers.BatchNormalization(epsilon=0.00001,momentum=0.9)(conv_3)

# Prediction
flatten = keras.layers.Flatten(name='flatten')(norm_3)
pred = keras.layers.Dense(10,name='pred')(flatten)

# Create the Keras model
model = keras.Model(
                    inputs=input_layer,
                    outputs=pred
                )

print(model.summary())

""" Train the model """
# Compile for training
model.compile(
            optimizer=keras.optimizers.SGD(learning_rate=0.001,momentum=0.9,nesterov=True),
            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,name='loss'),
            metrics=['accuracy']
            )

# Stop the model once the validation accuracy stops going down
earlystop_callback = tf.keras.callbacks.EarlyStopping(
                            monitor='val_accuracy',
                            mode='max',
                            patience=5,
                            restore_best_weights=True
                        )

# Train the model
# model.fit(
#         train_images, train_labels, epochs=200,
#         validation_data=(test_images, test_labels),
#         batch_size=200,
#         callbacks=[earlystop_callback],
#         verbose=1
#         )

# tf.keras.models.save_model(model=model,filepath='./seq/model.h5')


""" Evaluate model efficiency """
# Attach StateCapture layers to the model
model_after = tf.keras.models.load_model(str(Path('.').joinpath('seq').joinpath('model.h5')), compile=True)
model_before = tf.keras.models.load_model(str(Path('.').joinpath('seq').joinpath('model.h5')), compile=True)
efficiency_model_after = ts.build_efficiency_model(model=model_after,attach_to=['InputLayer','MaxPooling2D','Conv2D','Dense'],method='after', storage_path='./after/')
efficiency_model_before = ts.build_efficiency_model(model=model_before,attach_to=['InputLayer','MaxPooling2D','Conv2D','Dense'],method='before', storage_path='./before/')

# Collect the states for each layer
print()
print('Running model predictions to capture states...')
start = time.time()
predictions_after = efficiency_model_after.predict(test_images,batch_size=200)
predictions_before = efficiency_model_before.predict(test_images,batch_size=200)
print('Finished in {:.3f}s!'.format(time.time() - start))

# test_images = test_images[:,:,:,:]

# Initialize the distributions. 
# This is what we need in order to calculate the Mutual Information

mutual_info = []
normalized_mutual_info = []

print()

for layer in model_after.layers:
    print("")
    if layer.name == "input":
        continue
    elif "conv" in layer.name:
        print(layer.name)
        px_d = {}
        py_d = {}
        pxy_d = {}
        
        # We need the weights and biases that are applied to the input
        weights = (layer.get_weights()[0]).T
        weight_shape = weights.shape
        print("Weight's Shape: ", weight_shape)
        num_kernels = weight_shape[0]
        kernels = {}
        biases = layer.get_weights()[1]
        print("BIAS SHAPE", biases.shape)
        for i in range(num_kernels):
            kernels[i+1] = np.ravel(weights[i, :, :, :].T)
        
        # We need the filter size. 
        # At least for the first convolutional layer, for every image (z), we are going through a 5x5 tile of the image
        windowsize_x = weight_shape[-1]
        windowsize_y = weight_shape[-2]
        windowsize_z = weight_shape[-3]
        
        test_image_shape = test_images.shape
        print("WINDOW SIZES (x,y,z): ", windowsize_x, windowsize_y, windowsize_z)
        print("Test Image Shape",test_image_shape)

        dist_count = test_image_shape[0]*test_image_shape[1]*test_image_shape[2]
        
        # Initialize the output of the convolutional layer
        # outputimage = np.zeros((test_image_shape[0], test_image_shape[1] - windowsize_y + 1, test_image_shape[2] - windowsize_x + 1, num_kernels))
        outputimage = np.zeros((test_image_shape[0], test_image_shape[1] - windowsize_y + 1, test_image_shape[2] - windowsize_x + 1, num_kernels))
        print("OUTPUT IMAGE SHAPE: ", outputimage.shape)

        

        # for z in range(0,test_images.shape[0] - windowsize_z + 1, 1):
        for image in range(0,test_images.shape[0], 1):
            for y in range(0,test_images.shape[1] - windowsize_y + 1, 1):
                for x in range(0,test_images.shape[2] - windowsize_x + 1, 1):
                    
                    # we are iterating through the input (1x5x5)
                    input = test_images[image, y:y+windowsize_y, x:x+windowsize_x, :]
                    flatinput = np.ravel(input)

                    inputlist = flatinput
                    inputlist[inputlist > 0] = 1
                    inputlist[inputlist <= 0] = 0
                    inputlist.astype('uint8')
                    inputstring = ""
                    for i in inputlist:
                        inputstring = inputstring + str(round(i))

                    inputstring = str(inputstring)

                    # add input to the distribution
                    if inputstring in px_d.keys():
                        px_d[inputstring] = px_d[inputstring] + 1/dist_count
                    else:
                        px_d[inputstring] = 1/dist_count
                    
                    # every input gets mapped to 20 different outputs, which is a series of 0 and 1.  
                    outputstring = ""

                    # The depth of the output is actually the different neurons that are applied to the input. 
                    # Every input gets mapped to 20 different outputs.
                    for kernel in kernels.keys():
                        # The input gets multiplied by 20 different kernels.  
                        flat_kernel = np.reshape(kernels[kernel], (1,windowsize_y,windowsize_x, test_images.shape[-1]), order='C')
                        output = np.multiply(input, flat_kernel).astype("float64")
                        output[output == -0] = 0

                        # Then you add the bias to that kernel
                        outputsum = np.sum(output) + biases[kernel - 1]

                        # This is how we map the input to the outputs 
                        outputimage[image, y, x, kernel - 1] = outputsum

                        # Heres how you figure out if each of those operation is whether a 0 or 1. 
                        if outputsum > 0:
                            outputstring = outputstring + str(1)
                        else:
                            outputstring = outputstring + str(0)

                    if outputstring in py_d.keys():
                        py_d[outputstring] = py_d[outputstring] + 1/dist_count
                    else:
                        py_d[outputstring] = 1/dist_count

                    # Create a distribution of how each input gets mapped to 20 different outputs. 
                    pxykey = (inputstring, outputstring)
                    if pxykey in pxy_d.keys():
                        pxy_d[pxykey] = pxy_d[pxykey] + 1/dist_count
                    else:
                        pxy_d[pxykey] = 1/dist_count


        # Change the test_image.  This is what your method should return. So it can be used as an input for the next layer too.  
        test_images = outputimage
        # MI = 0
        # for i in px_d.keys():
        #     for j in py_d.keys():
        #         joint = (i, j)
        #         if joint in pxy_d.keys():
        #             add = pxy_d[joint]*math.log2(pxy_d[joint]/(px_d[i]*py_d[j]))
        #             MI = MI + add
        # print("MUTUAL INFORMATION: ", MI)
        # Mutual_info.append(MI)
        
        shannons_input = 0
        for shan in px_d.keys():
            shannons_input = shannons_input + px_d[shan]*math.log2(px_d[shan])
        shannons_input = shannons_input * -1

        shannons_output = 0
        for shan in py_d.keys():
            shannons_output = shannons_output + py_d[shan]*math.log2(py_d[shan])
        shannons_output = shannons_output * -1

        print("INPUT DISTRIBUTION")
        print(px_d)

        print("OUTPUT DISTRIBTUION")
        print(py_d)

        print("INPUT AND OUTPUT DISTRIBUTION")
        print(pxy_d)

        MI = 0
        for ij in pxy_d.keys():
            i = ij[0]
            j = ij[1]
            add = pxy_d[ij]*math.log2(pxy_d[ij]/(px_d[i]*py_d[j]))
            MI = MI + add
        norm_MI = 2*MI/(shannons_input + shannons_output)
        mutual_info.append(MI)
        normalized_mutual_info.append(norm_MI)
        print("MUTUAL INFORMATION: ", MI)

        


    elif "batch" in layer.name:
        print(layer.name)
        gammas = [[layer.gamma.numpy()]]
        betas = [[layer.beta.numpy()]]
        imageshape = test_images.shape
        for image in range(imageshape[0]):
            
            input = test_images[image,:,:,:].flatten()
            len = input.shape

            mean = np.sum(input)/len
            variance = np.sum((input - mean)**2)/len

            input = ((input - mean)/math.sqrt(variance + layer.epsilon))
            input = np.reshape(input, imageshape[1:], order='C')
            input = input * gammas + betas
            test_images[image, :, :, :] = input

    elif "max" in layer.name: 
        print(layer.name)
        pool_size = layer.pool_size
        windowsize_x = pool_size[1]
        windowsize_y = pool_size[0]

        imageshape = test_images.shape
        print("MAX POOL LAYER IMAGESHAPE", imageshape)
        outputimage = np.zeros((imageshape[0], int(imageshape[1]/windowsize_y), int(imageshape[2]/windowsize_x), imageshape[-1]))
        print("OUTPUTIMAGE SHAPE: ", outputimage.shape)

        for image in range(imageshape[0]):
            input = test_images[image,:,:,:]
            inputshape = input.shape
            for y in range(0, inputshape[0], windowsize_y):
                for x in range(0, inputshape[1], windowsize_x):
                    inp = input[y:y+windowsize_y, x:x+windowsize_x]
                    out = np.max(inp, axis=(0,1))
                    outputimage[image,int(y/windowsize_y), int(x/windowsize_x),:] = out
        test_images = outputimage
        print(test_images.shape)
    else:
        print(layer.name)
        print("MUTUAL INFORMATION", mutual_info)
        print("NORMALIZED MUTUAL INFORMATION", normalized_mutual_info)



# print("INPUT DISTRIBUTION")
# print(px_d)

# print("OUTPUT DISTRIBUTION")
# outputcount = 0
# for key in py_d.keys():
#     outputcount = outputcount + py_d[key]
# print("OUTPUT COUNT", outputcount)
# print(py_d)

# print("INPUT AND OUTPUT DISTRIBUTION")
# print(pxy_d)

# layerinfo = []
# num_ofstates = [[], []]
# layer_names = [[], []]
# layer_eff = [[], []]
# state_freqs = [[],[]]
# state_ids = [[], []]
# uniq_states = [[], []]
# micro_states = [[], []]
# max_entropy = [[], []]
# numoflayers = 0

# print()
# print('Getting the number of states in before each layer...')
# for layer in efficiency_model_before.efficiency_layers:
#     entropy_dict = {}
#     max_entropy[0].append(asc.max_entropy(layer))
#     layer_names[0].append(layer.name)
#     num_ofstates[0].append(layer.state_count)
#     layer_eff[0].append(100*layer.efficiency())
#     state_freqs[0].append(asc.counts(layer))
#     state_ids[0].append(asc.state_ids(layer))
#     uniq_states[0].append(len(state_freqs[0][-1]))
#     micro_states[0].append(state_freqs[0][-1].sum())
#     numoflayers = numoflayers+1

# print()
# print('Getting the number of states in after each layer...')
# print()
# for layer in efficiency_model_after.efficiency_layers:
#     entropy_dict = {}
#     max_entropy[-1].append(asc.max_entropy(layer))
#     layer_names[-1].append(layer.name)
#     num_ofstates[-1].append(layer.state_count)
#     layer_eff[1].append(100*layer.efficiency())
#     #layer.states will return numpy array with boolean values
#     state_freqs[-1].append(asc.counts(layer))
#     state_ids[-1].append(asc.state_ids(layer))
#     uniq_states[-1].append(len(state_freqs[1][-1]))
#     micro_states[1].append(state_freqs[1][-1].sum())

# numoflayers = 1
# for i in range(numoflayers):
#     print("Layer ", i+1)
#     print("Layer Names: ",layer_names[0][i], "|", layer_names[1][i])
#     print("Number of States: ",num_ofstates[0][i], "|", num_ofstates[1][i])
#     print("Layer Efficiency (Percent): ", layer_eff[0][i], "|", layer_eff[1][i])
#     print("Number of Unique States: ", uniq_states[0][i], "|", uniq_states[1][i])
#     print("Number of Microstates: ", micro_states[0][i], "|", micro_states[1][i])
#     print("Max Entropy: ", max_entropy[0][i], "|", max_entropy[1][i]) #number of neurons
#     print("")

#     print("BEFORE")
#     entropy_before = []
#     probs_before = []
#     for state in range(0, uniq_states[0][i]):
#         bytearr = bytearray(state_ids[0][i][state])
#         bytearr = np.array([bytearr])
#         decompress = TensorState.States.decompress_states(bytearr, max_entropy[0][i])
        
#         unqiue_freq_before = state_freqs[0][i][state]
#         total_states_before = micro_states[0][i]
#         prob_before = unqiue_freq_before/total_states_before
#         probs_before.append(prob_before)
#         shan_entropy_before = shans_entropy(prob=prob_before)
#         # print("Shannons Entropy", shan_entropy_before)

#         entropy_before.append(shan_entropy_before)
#         # print("BEFORE TENSORSTATE", unqiue_freq_before, decompress, prob_before, shan_entropy_before)

#     entropy_after = []
#     probs_after =[]
#     print("AFTER")
#     for state in range(0, uniq_states[1][i]):
#         bytearr = bytearray(state_ids[1][i][state])
#         bytearr = np.array([bytearr])
#         decompress = TensorState.States.decompress_states(bytearr, max_entropy[1][i])

#         unique_freq_after = state_freqs[1][i][state]
#         total_states_after = micro_states[1][i]
#         prob_after = unique_freq_after/total_states_after
#         probs_after.append(prob_after)
#         shan_entropy_after = shans_entropy(prob=prob_after)
#         # print("Shannons Entropy", shan_entropy_after)

#         entropy_after.append(shan_entropy_after)
#         print(unique_freq_after, decompress, prob_after, shan_entropy_after)
#         # print("TensorState (count): ", unique_freq_after, decompress)


    
