import os
from pathlib import Path
import math

import multiprocessing, argparse, logging, matplotlib, copy, imageio
matplotlib.use('agg')
import matplotlib.pyplot as plt
from pathlib import Path
from pylab import figure, axes, pie, title, show
import binascii
import codecs 
import json
import zarr, abc, sys, logging
from numcodecs import Blosc
import TensorState.States
from scipy import stats
from scipy.stats import chi2_contingency
from mpl_toolkits import mplot3d
import matplotlib.pyplot as plt
from scipy.stats import multivariate_normal
from sklearn.feature_selection import mutual_info_classif
from sklearn.metrics import mutual_info_score
import pandas as pd

# from keras.callbacks import LambdaCallback

# Set the log level to hide some basic warning/info generated by Tensorflow
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

# Fix for cudnn error on RTX gpus
os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'

import tensorflow as tf
import tensorflow.keras as keras
from tensorflow.keras.models import load_model
import TensorState as ts
import numpy as np
# np.set_printoptions(threshold=sys.maxsize)
import time
from TensorState import AbstractStateCapture as asc
from TensorState.Layers import StateCapture, StateCaptureHook
# import TensorState.States as tensorstate
import struct



def shans_entropy(prob):
    entropy = -(prob * np.log2(prob))
    return entropy

def calc_MI(x, y, bins):
    c_xy = np.histogram2d(x, y, bins)[0]
    g, p, dof, expected = chi2_contingency(c_xy, lambda_="log-likelihood")
    mi = 0.5 * g / c_xy.sum()
    return mi

def mutual_info(x, y, xy):
    print(math.log(xy/(x*y)))
    mi = xy*(math.log(xy/(x*y)))
    return mi

outputim = Path("/home/ec2-user/aIQ/image")

""" Load MNIST and transform it """
# Load the data
mnist = keras.datasets.mnist
(train_images,train_labels), (test_images,test_labels) = mnist.load_data()

# Normalize the data
train_images = train_images/255
test_images = test_images/255

# Add a channel axis
train_images = train_images[..., tf.newaxis]
test_images = test_images[..., tf.newaxis]

test_images = test_images[0:1,:,:,:]
test_labels = test_labels[0:1]
testmin = np.amin(test_images)

# print("test minimum", testmin)
test_images = test_images - testmin
# print(test_images > 0)

print(test_images.shape)
print(test_labels.shape)
""" Create a LeNet-5 model """
# Set the random seed for reproducibility
tf.random.set_seed(0)

# Set the convolutional layer settings
reg = keras.regularizers.l2(0.0005)
kwargs = {'activation': 'elu',
        'kernel_initializer': 'he_normal',
        'kernel_regularizer': reg,
        'bias_regularizer': reg}

# Build the layers
input_layer = keras.layers.Input(shape=(28,28,1), name='input')

# Unit 1
conv_1 = keras.layers.Conv2D(20, 5, name='conv_1',**kwargs)(input_layer)
norm_1 = keras.layers.BatchNormalization(epsilon=0.00001,momentum=0.9)(conv_1)
maxp_1 = keras.layers.MaxPool2D((2,2), name='maxp_1')(norm_1)

# Unit 2
conv_2 = keras.layers.Conv2D(50, 5, name='conv_2', **kwargs)(maxp_1)
norm_2 = keras.layers.BatchNormalization(epsilon=0.00001,momentum=0.9)(conv_2)
maxp_2 = keras.layers.MaxPool2D((2,2), name='maxp_2')(norm_2)

# Fully Connected
conv_3 = keras.layers.Conv2D(100, 4, name='conv_3', **kwargs)(maxp_2)
norm_3 = keras.layers.BatchNormalization(epsilon=0.00001,momentum=0.9)(conv_3)

# Prediction
flatten = keras.layers.Flatten(name='flatten')(norm_3)
pred = keras.layers.Dense(10,name='pred')(flatten)

# Create the Keras model
model = keras.Model(
                    inputs=input_layer,
                    outputs=pred
                )

print(model.summary())

""" Train the model """
# Compile for training
model.compile(
            optimizer=keras.optimizers.SGD(learning_rate=0.001,momentum=0.9,nesterov=True),
            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,name='loss'),
            metrics=['accuracy']
            )

# Stop the model once the validation accuracy stops going down
earlystop_callback = tf.keras.callbacks.EarlyStopping(
                            monitor='val_accuracy',
                            mode='max',
                            patience=5,
                            restore_best_weights=True
                        )

# Train the model
# model.fit(
#         train_images, train_labels, epochs=200,
#         validation_data=(test_images, test_labels),
#         batch_size=200,
#         callbacks=[earlystop_callback],
#         verbose=1
#         )

# tf.keras.models.save_model(model=model,filepath='./seq/model.h5')


""" Evaluate model efficiency """
# Attach StateCapture layers to the model
model_after = tf.keras.models.load_model(str(Path('.').joinpath('seq').joinpath('model.h5')), compile=True)
model_before = tf.keras.models.load_model(str(Path('.').joinpath('seq').joinpath('model.h5')), compile=True)
efficiency_model_after = ts.build_efficiency_model(model=model_after,attach_to=['InputLayer','MaxPooling2D','Conv2D','Dense'],method='after', storage_path='./after/')
efficiency_model_before = ts.build_efficiency_model(model=model_before,attach_to=['InputLayer','MaxPooling2D','Conv2D','Dense'],method='before', storage_path='./before/')

# Collect the states for each layer
print()
print('Running model predictions to capture states...')
start = time.time()
predictions_after = efficiency_model_after.predict(test_images,batch_size=200)
predictions_before = efficiency_model_before.predict(test_images,batch_size=200)
print('Finished in {:.3f}s!'.format(time.time() - start))


# alpha_vals = range(0, 10000)

kernel_vals = {}
whichlayer = 0


print("FIRST LAYER BIAS")
firstlayerbias = (model_after.get_layer("conv_1").bias).numpy()

# THIS WORKS
# for layer in model_after.layers:
#     try:
#         weights = layer.get_weights()
#         print("weight", weights[0])
#         print("bias", weights[1])
#     except:
#         continue

for layer in model_after.get_weights():
    if len(layer.shape) > 1:
        print(layer.shape)
        n_rows = layer.shape[0]
        n_cols = layer.shape[1]
        n_depths = layer.shape[2]
        n_neurons = layer.shape[3]
        if n_neurons == 20:
            for row in range(n_rows):
                for col in range(n_cols):
                    for depth in range(n_depths):
                        for neuron in range(n_neurons):
                            if row == 0 and col == 0:
                                kernel_vals['Filter ' + str(neuron + 1)] = []
                            kernel_vals['Filter ' + str(neuron + 1)].append(layer[row][col][depth][neuron])
    else:
        break

print(kernel_vals)
print(" ")

windowsize_r = 5
windowsize_c = 5

truevals = 0
falsevals = 0

inputtruevals = 0
inputfalsevals = 0

test_images = test_images.squeeze()
print(test_images.shape)
print(" ")

outputtruefalse_series = {}
inputtruefalse_series = {}
inputstring_dict = {}

inputcount = 0
outputcount = 0
pxycount = 0

pxydict = {}
firstkey = list(kernel_vals.keys())[0]
for r in range(0,test_images.shape[0] - windowsize_r + 1, 1):
    for c in range(0,test_images.shape[1] - windowsize_c + 1, 1):
        inputtruefalsekey = ""
        input = test_images[r:r+windowsize_r,c:c+windowsize_c]

        truefalsekey = ""
        filtercount = 1
        inputstring = ""
        
        for filter in kernel_vals:

            if filter == firstkey:
                for inputval in input.flatten():
                    inputtruefalsekey = ""
                    if inputval > 0 or (inputval > -0):
                        inputtruevals = inputtruevals + 1
                        inputtruefalsekey = inputtruefalsekey + "1"
                        inputstring = inputstring + "1"
                    else:
                        inputfalsevals = inputfalsevals + 1
                        inputtruefalsekey = inputtruefalsekey + "0"
                        inputstring = inputstring + "0"
                    if inputtruefalsekey in inputtruefalse_series.keys():
                        inputtruefalse_series[inputtruefalsekey] = inputtruefalse_series[inputtruefalsekey] + 1
                    else:
                        inputtruefalse_series[inputtruefalsekey] = 1
                inputcount = inputcount + 1
                if inputstring in inputstring_dict.keys():
                    inputstring_dict[inputstring] = inputstring_dict[inputstring] + 1
                else:
                    inputstring_dict[inputstring] = 1
                # print("Entire Input ({}, {}) Simplified (before weights&bias): {}".format(r+1, c+1, inputstring))
                # print(input)
                # print("Input Sum: {}".format(np.sum(input)))
            filter = np.reshape(kernel_vals[filter], (5,5), order='C')
            output = np.multiply(input, filter).astype("float64")
            outputsum = np.sum(output) + firstlayerbias[filtercount - 1]
            if (outputsum > int(0)) or (outputsum > int(-0)):
                truevals = truevals + 1
                truefalsekey = truefalsekey + "1"
            else:
                falsevals = falsevals + 1
                truefalsekey = truefalsekey + "0"
            # print("\t Neuron {} Output: {} -- {}".format(filtercount, outputsum, truefalsekey[-1]))
            filtercount = filtercount + 1

        # for char in inputstring:
        pxydist = (inputstring, truefalsekey)
        if pxydist in pxydict:
            pxydict[pxydist] = pxydict[pxydist] + 1
        else:
            pxydict[pxydist] = 1
        pxycount = pxycount + 1

        if truefalsekey in outputtruefalse_series.keys():
            outputtruefalse_series[truefalsekey] = outputtruefalse_series[truefalsekey] + 1
        else:
            outputtruefalse_series[truefalsekey] = 1
        outputcount = outputcount + 1

        
        # print(" ")

print(" ")
print("P(xy): DISTRIBUTION (input&output)")
print(len(pxydict))
print(pxycount)
for item in pxydict.keys():
    pxydict[item] = pxydict[item]/pxycount
print(pxydict)
print(" ")

# print("P(x): DISTRIBUTION (input)")

# print(len(inputtruefalse_series))
# print(inputcount)
# print(inputtruefalse_series)
# print("")

print("P(x): DISTRIBUTION (input)")

print(len(inputstring_dict))
print(inputcount)
for item in inputstring_dict.keys():
    inputstring_dict[item] = inputstring_dict[item]/inputcount
print(inputstring_dict)
print("")


print("P(y): DISTRIBUTION (output)")
print(len(outputtruefalse_series))
print(outputcount)
for item in outputtruefalse_series.keys():
    outputtruefalse_series[item] = outputtruefalse_series[item]/outputcount
print(outputtruefalse_series)

a = np.zeros(shape = (len(outputtruefalse_series), len(pxydict)))
df = pd.DataFrame(a, columns=pxydict.keys(), index=outputtruefalse_series.keys())
# df.set_index(inputstring_dict.keys())


for joint in pxydict.keys():
    df.at[joint[1], joint[0]] = pxydict[joint]
print(df)
df.plot.kde().plot()
# ax = fig.axes
# print("ax", ax)
plt.xlim(left=-.005, right=.02)
plt.ylim(bottom=0, top=20)
plt.gca().get_legend().remove()
plt.savefig('plotpic.png')


"""THIS DOESN'T WORK BECAUSE IM TAKING LOG(0) SOMETIMES"""
# no_counts = 0
# summations = 0
# hx = 0
# hy = 0
# for px in inputstring_dict.keys():
#     for py in outputtruefalse_series.keys():
#         joint = (px, py)
#         log = 0
#         # print(px, py, (px, py))
        
#         pxval = inputstring_dict[px]/576
#         pyval = outputtruefalse_series[py]/576

#         hxprob = pxval*math.log(pxval, 2)
#         hx = hx + hxprob

#         hyprob = pyval*math.log(pyval, 2)
#         hy = hy + hyprob
#         print(pxval, pyval)
        
        

        # if joint in pxydict.keys():
        #     # print("\t", inputstring_dict[px], outputtruefalse_series[py], pxydict[joint])
        #     pxyval = pxydict[joint]/576
        #     log = math.log(pxyval/(pxval*pyval), 2)
        #     log = pxyval*log
        #     # log = math.log(pxydict[joint]/(inputtruefalse_series[px]*outputtruefalse_series[py]), 2)
        #     # log = pxydict[joint]*log
        #     summations = summations + log
        # else:

        #     # print("\t", inputstring_dict[px], outputtruefalse_series[py], 0)
        #     # print("Not here")
        #     # print(pxval, pyval)
        #     no_counts = no_counts + 1
        #     # # summation = pxyval*math.log(0/(pxval*pyval),2)
        #     # # print("DOESN'T WORK HERE", pxyval, pxval, pyval)
        #     # log = 0.5 * math.log(.5,2)

# print("Number of combinations that do not work: ", no_counts)
# print("Mutual Information: ", summations)
# print("Mutual Information should less than the minimum Shannon's Entropy: ", -1*hx, -1*hy)
        

# marginalx = {}
# marginaly = {}
# for joint in pxydict.keys():
#     pxkey = joint[0]
#     pykey = joint[1]



    # if pxkey in marginalx.keys():
    #     marginalx[pxkey] = marginalx[pxkey] + pxydict[joint]
    # else:
    #     marginalx[pxkey] = pxydict[joint]

    # if pykey in marginaly.keys():
    #     marginaly[pykey] = marginaly[pykey] + pxydict[joint]
    # else:
    #     marginaly[pykey] = pxydict[joint]

    

# print("MARGINALX")
# print(marginalx)
# print("MARGINALY")
# print(marginaly)
    # loginparen = pxydict[joint]/(inputstring_dict[pxkey]*outputtruefalse_series[pykey])
    # log = math.log(loginparen, 2)
    # print(joint, log)
        




print()
                
# hist = np.histogram(window,bins=grey_levels)
# print(model_after.get_config())
# for layer in model_after.layers:
# #     # if layer.name == "input":
# #     #     continue
# #     print(layer)
# #     print(layer.name)
# #     # conv1 is 5x5x20
#     print(layer.weights)
# #     # filters = np.asarray(filters)
# #     # print(len(filters.flatten())) 
#     break

# for layer in model_after.layers:
#     # if layer.name == "input":
#     #     continue
#     print(layer)
#     print(layer.name)
#     filters, biases = layer.get_weights()
#     print(filters)
#     break



layerinfo = []
num_ofstates = [[], []]
layer_names = [[], []]
layer_eff = [[], []]
state_freqs = [[],[]]
state_ids = [[], []]
uniq_states = [[], []]
micro_states = [[], []]
max_entropy = [[], []]
numoflayers = 0

print()
print('Getting the number of states in before each layer...')
for layer in efficiency_model_before.efficiency_layers:
    entropy_dict = {}
    max_entropy[0].append(asc.max_entropy(layer))
    layer_names[0].append(layer.name)
    num_ofstates[0].append(layer.state_count)
    layer_eff[0].append(100*layer.efficiency())
    state_freqs[0].append(asc.counts(layer))
    state_ids[0].append(asc.state_ids(layer))
    uniq_states[0].append(len(state_freqs[0][-1]))
    micro_states[0].append(state_freqs[0][-1].sum())
    numoflayers = numoflayers+1

print()
print('Getting the number of states in after each layer...')
print()
for layer in efficiency_model_after.efficiency_layers:
    entropy_dict = {}
    max_entropy[-1].append(asc.max_entropy(layer))
    layer_names[-1].append(layer.name)
    num_ofstates[-1].append(layer.state_count)
    layer_eff[1].append(100*layer.efficiency())
    #layer.states will return numpy array with boolean values
    state_freqs[-1].append(asc.counts(layer))
    state_ids[-1].append(asc.state_ids(layer))
    uniq_states[-1].append(len(state_freqs[1][-1]))
    micro_states[1].append(state_freqs[1][-1].sum())


for i in range(numoflayers):
    print("Layer ", i+1)
    print("Layer Names: ",layer_names[0][i], "|", layer_names[1][i])
    print("Number of States: ",num_ofstates[0][i], "|", num_ofstates[1][i])
    print("Layer Efficiency (Percent): ", layer_eff[0][i], "|", layer_eff[1][i])
    print("Number of Unique States: ", uniq_states[0][i], "|", uniq_states[1][i])
    print("Number of Microstates: ", micro_states[0][i], "|", micro_states[1][i])
    print("Max Entropy: ", max_entropy[0][i], "|", max_entropy[1][i]) #number of neurons
    print("")

    print("BEFORE")
    entropy_before = []
    probs_before = []
    for state in range(0, uniq_states[0][i]):
        bytearr = bytearray(state_ids[0][i][state])
        bytearr = np.array([bytearr])
        decompress = TensorState.States.decompress_states(bytearr, max_entropy[0][i])
        
        unqiue_freq_before = state_freqs[0][i][state]
        total_states_before = micro_states[0][i]
        prob_before = unqiue_freq_before/total_states_before
        probs_before.append(prob_before)
        shan_entropy_before = shans_entropy(prob=prob_before)
        # print("Shannons Entropy", shan_entropy_before)

        entropy_before.append(shan_entropy_before)
        # print("BEFORE TENSORSTATE", unqiue_freq_before, decompress, prob_before, shan_entropy_before)

    entropy_after = []
    probs_after =[]
    print("AFTER")
    for state in range(0, uniq_states[1][i]):
        bytearr = bytearray(state_ids[1][i][state])
        bytearr = np.array([bytearr])
        decompress = TensorState.States.decompress_states(bytearr, max_entropy[1][i])

        unique_freq_after = state_freqs[1][i][state]
        total_states_after = micro_states[1][i]
        prob_after = unique_freq_after/total_states_after
        probs_after.append(prob_after)
        shan_entropy_after = shans_entropy(prob=prob_after)
        # print("Shannons Entropy", shan_entropy_after)

        entropy_after.append(shan_entropy_after)
        # print(unique_freq_after, decompress, prob_after, shan_entropy_after)
        # print("TensorState (count): ", unique_freq_after, decompress)
    
    joint_entropies = []
    joint_probs = []
    MI = []
    count = 0 
    for i_before in range(0, len(probs_before)):
        for i_after in range(0, len(probs_after)):
            joint_prob = probs_before[i_before]*probs_after[i_after]
            joint_entropy = shans_entropy(prob=joint_prob)
            joint_entropies.append(joint_entropy)
            mi = entropy_before[i_before]+entropy_after[i_after]-joint_entropy
            count = count + 1
            MI.append(mi)
            # print(i_before+1, i_after+1)
            # print(entropy_before[i_before], entropy_after[i_after], joint_entropy, mi)

    # print("PointWise Mutual Information: ", sum(MI)/count)
    # print("Matches Layer Efficiency: ", sum(entropy_before)/max_entropy[0][i], sum(entropy_after)/max_entropy[1][i])
    # calc_jointentropy = (sum(entropy_before)/max_entropy[0][i])*(sum(entropy_after)/max_entropy[1][i])
    # print("Calculated Mutual Information for Layer: ", sum(entropy_before)/max_entropy[0][i] + sum(entropy_after)/max_entropy[1][i] - calc_jointentropy)
    # print("Mutual Information", layer_names[0][i], layer_names[1][i], sum_mi/count)
    print(" ")
    print(" ")
    break



# print("Layer Names: ", layer_names)
# print("Number of States: ", num_ofstates)
# print("Layer Efficiency: ", layer_eff)
# print("Number of Unique States: ", uniq_states)
# print("Number of Microstates: ", micro_states)
    # for alpha in alpha_vals:
    #     entropy_val = asc.entropy(layer, alpha=alpha)
    #     entropy_dict[alpha] = entropy_val
    #     if math.isinf(entropy_val):
    #         break

    # plt.subplot(2,2,i)
    # plt.plot(*zip(*sorted(entropy_dict.items())))
    # plt.title(layer.name)
    # plt.xlabel("Alpha Values")
    # plt.ylabel("Entropy")
    # plt.scatter(0, entropy_dict[0])
    # plt.scatter(1, entropy_dict[1])
    # plt.scatter(2, entropy_dict[2])
    # plt.scatter(alpha-1, entropy_dict[alpha-1])

    # i = i + 1
    # print(" ")

# plt.tight_layout()
# plt.savefig('image.png')

# # Calculate each layers efficiency
# print()
# print('Evaluating efficiency of each layer...')
# for layer in efficiency_model.efficiency_layers:
#     start = time.time()
#     print('Layer {} efficiency: {:.1f}% ({:.3f}s)'.format(layer.name,100*layer.efficiency(),time.time() - start))

# # Calculate the aIQ
# beta = 2 # fudge factor giving a slight bias toward accuracy over efficiency

# print()
# print('Network metrics...')
# print('Beta: {}'.format(beta))

# network_efficiency = ts.network_efficiency(efficiency_model)
# print('Network efficiency: {:.1f}%'.format(100*network_efficiency))

# accuracy = np.sum(np.argmax(predictions,axis=1)==test_labels)/test_labels.size
# print('Network accuracy: {:.1f}%'.format(100*accuracy))

# aIQ  = ts.aIQ(network_efficiency,accuracy,beta)
# print('aIQ: {:.1f}%'.format(100*aIQ))
